{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7858154,"sourceType":"datasetVersion","datasetId":4609285},{"sourceId":7869861,"sourceType":"datasetVersion","datasetId":4617728},{"sourceId":7869969,"sourceType":"datasetVersion","datasetId":4617807},{"sourceId":7937599,"sourceType":"datasetVersion","datasetId":4666369}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"096dbe33c55126c5d8db0f1b650aa2af6bd938a7d34f117fe790662238af3888"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport sklearn\nfrom sklearn.impute import KNNImputer,SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T18:36:51.713506Z","iopub.execute_input":"2024-04-08T18:36:51.713969Z","iopub.status.idle":"2024-04-08T18:36:51.722258Z","shell.execute_reply.started":"2024-04-08T18:36:51.713933Z","shell.execute_reply":"2024-04-08T18:36:51.721006Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\n\ninfo = {\n    \"MUSTERI_ID\" : \"Müşterileri ayırt eden unique ID'ler.\",\n    \"LABEL\": \"Müşterinin aldığı Hayat sigortası ürününün çeşidi.\",\n    \"FLAG\":\"Verinin ait olduğu ay.\",\n    \"PP_CINSIYET\": \"Sözleşme sahibinin cinsiyeti.1: Erkek 2: Kadın\",\n    \"PP_YAS\" : \"Sözleşme sahibinin ay bazlı olarak yaşı.\",\n    \"PP_MESLEK\" : \"Sözleşme sahibinin mesleği.\",\n    \"PP_MUSTERI_SEGMENTI\" : \"Sözleşme sahibinin müşteri segmenti. 101: A segment, 102: B segment,103: C segment, 104: D segment,105: E segment ,106: F segment\",\n    \"PP_UYRUK\": \"Sözleşme sahibinin uyruk bilgisi. 1:TC Vatandaşı , 2:Mavi Kart, 3:Yabancı Uyruklu\",\n    \"IL\":\"Sözleşme sahiplerinin yaşadığı illere ait plaka kodu.( 0 = Yurtdışı)\",\n    \"SORU_YATIRIM_KARAKTERI_CVP\": \"Sözleşme sahibinin ankete verdiği cevaplara göre belirlenen yatırım karakteri.\",\n    \"SORU_YATIRIM_KARAKTERI_RG\": \"Sözleşme sahibinin ankette verdiği cevabın üstünden geçen süre (ay).\",\n    \"SORU_MEDENI_HAL_CVP\": \"Sözleşme sahibinin ankete verdiği cevaba göre medeni durumu.\",\n    \"SORU_MEDENI_HAL_RG\":\" Sözleşme sahibinin ankette verdiği cevabın üstünden geçen süre (ay).\",\n    \"SORU_EGITIM_CVP\": \"Sözleşme sahibinin ankete verdiği cevaplara göre eğitim durumu.\",\n    \"SORU_EGITIM_RG\": \"Sözleşme sahibinin ankette verdiği cevabın üstünden geçen süre (ay).\",\n    \"SORU_GELIR_CVP\": \"Sözleşme sahibinin ankete verdiği cevaba göre gelir durumu.\",\n    \"SORU_GELIR_RG\": \"Sözleşme sahibinin ankette verdiği cevabın üstünden geçen süre (ay).\",\n    \"SORU_COCUK_SAYISI_CVP\": \"Sözleşme sahibinin ankete verdiği cevaba göre çocuk sayısı.\",\n    \"SORU_COCUK_SAYISI_RG\": \"Sözleşme sahibinin ankette verdiği cevabın üstünden geçen süre (ay).\",\n    \"BES_AYRILMA_TALEP_ADET\" : \"Sözleşme sahibinin BES hesabından ayrılmak için açtığı talep sayısı.\",\n    \"ODEMEME_TALEP_ADET\" : \"Sözleşme sahibinin son 1 sene içerisinde kaç kez ödememe talimatı verdiğini gösterir.\",\n    \"HAYAT_AYRILMA_TALEP_ADET\" : \"Sözleşme sahibinin Hayat sigortasından ayrılmak için açtığı talep sayısı.\",\n    \"BILGI_TALEP_ADET\" : \"Sözleşme sahibinin sözleşmesi için kaç kez bilgi talep ettiğini gösterir.\",\n    \"VADE_TUTAR_0 - VADE_TUTAR_11\" : \"Sözleşme sahibinin sahip olduğu ürünlerin son 12 aya ait toplam vade tutarları\",\n    \"ODEME_TUTAR_0 - ODEME_TUTAR_11 \": \"Sözleşme sahibinin sahip olduğu ürünler için son 12 ayda yaptığı ödeme tutarları\",\n    \"SON_AY_KATKI_MIKTARI\" : \"Sözleşme sahibinin son bir ay içinde yaptığı ek katkı ödemelerinin TL cinsinden toplam miktarı\",\n    \"SON_AY_KATKI_ADET\" : \"Sözleşme sahibinin son bir ay içinde yaptığı ek katkı ödemelerinin adedi\",\n    \"SON_CEYREK_KATKI_MIKTARI\" : \"Sözleşme sahibinin son üç ay içinde yaptığı ek katkı ödemelerinin TL cinsinden toplam miktarı\",\n    \"SON_CEYREK_KATKI_ADET\" : \"Sözleşme sahibinin son üç ay içinde yaptığı ek katkı ödemelerinin adedi\",\n    \"SON_SENE_KATKI_MIKTARI\" : \"Sözleşme sahibinin son bir sene içinde yaptığı ek katkı ödemelerinin TL cinsinden toplam miktarı\",\n    \"SON_SENE_KATKI_ADET\" : \"Sözleşme sahibinin son bir sene içinde yaptığı ek katkı ödemelerinin adedi\",\n    \"ANAPARA\": \"Sözleşme sahibinin TL cinsinden toplam yatırdığı para miktarı.\",\n    \"GETIRI\" : \"Sözleşme sahibinin TL cinsinden yatirdiği paradan elde ettiği getiri.\",\n    \"BU1 - BU24\": \"BES ÜRÜN 1 - 24, Kişinin kolonda belirtilen BES ürününe sahip olup olmama durumu (Ürün özellikleri ayrı bir dokumanda verilmiştir)\",\n    \"HU1 - HU19\": \"HAYAT ÜRÜN 1 - 19, Kişinin kolonda belirtilen Hayat ürününe sahip olup olmama durumu (Ürün özellikleri ayrı bir dokumanda verilmiştir)\",\n    \"AKTIF_ILK_POLICE_RG\": \"Sözleşme sahibinin aktif olan poliçeleri arasından en eskisinin üstünden geçen süre (ay).\"\n\n}\n\n\nclass Analysies():\n    def __init__(self,trainPath,testPath,excelPath, info= info):\n\n        self.trainData = pd.read_csv(trainPath)\n        self.testData = pd.read_csv(testPath)\n        self.productFeature = pd.read_excel(excelPath)\n        self.info = info\n\n    def grab_col_names(self,df,cat_th=20, car_th=25):\n        \"\"\"\n\n        Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.\n        Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.\n\n        Parameters\n        ------\n            df: df\n                    Değişken isimleri alınmak istenilen df\n            cat_th: int, optional\n                    numerik fakat kategorik olan değişkenler için sınıf eşik değeri\n            car_th: int, optinal\n                    kategorik fakat kardinal değişkenler için sınıf eşik değeri\n\n        Returns\n        ------\n            cat_cols: list\n                    Kategorik değişken listesi\n            num_cols: list\n                    Numerik değişken listesi\n            cat_but_car: list\n                    Kategorik görünümlü kardinal değişken listesi\n\n        Examples\n        ------\n            import seaborn as sns\n            df = sns.load_dataset(\"iris\")\n            print(grab_col_names(df))\n\n\n        Notes\n        ------\n            cat_cols + num_cols + cat_but_car = toplam değişken sayısı\n            num_but_cat cat_cols'un içerisinde.\n            Return olan 3 liste toplamı toplam değişken sayısına eşittir: cat_cols + num_cols + cat_but_car = değişken sayısı\n\n        \"\"\"\n\n\n    # cat_cols, cat_but_car\n        cat_cols = [col for col in df.columns if df[col].dtypes == \"O\"]\n        num_but_cat = [col for col in df.columns if df[col].nunique() < cat_th and\n                    df[col].dtypes != \"O\"]\n        cat_but_car = [col for col in df.columns if df[col].nunique() > car_th and\n                    df[col].dtypes == \"O\"]\n        cat_cols = cat_cols + num_but_cat\n        cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n        # num_cols\n        num_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n        num_cols = [col for col in num_cols if col not in num_but_cat]\n        \n       \n       \n\n\n        print(f\"Observations: {df.shape[0]}\")\n        print(f\"Variables: {df.shape[1]}\")\n        print(f'cat_cols: {len(cat_cols)}')\n        print(f'num_cols: {len(num_cols)}')\n        print(f'cat_but_car: {len(cat_but_car)}')\n        print(f'num_but_cat: {len(num_but_cat)}')\n        return cat_cols, num_cols, cat_but_car\n    def cat_summary(self,dataframe, col_name, plot=False):\n\n        for col in col_name:\n\n            \n            if col.startswith(\"BU\") is False or col.startswith(\"HU\") is False :\n                \n                print(pd.DataFrame({col: dataframe[col].value_counts(),\n                                    \"Ratio\": 100 * dataframe[col].value_counts() / len(dataframe),\n                                    \"Info\" : self.info[col]}))\n                \n                print(dataframe[col].dtype)\n\n                print(\"##########################################\")\n                \n            else:\n                print(pd.DataFrame({col: dataframe[col].value_counts(),\n                                    \"Ratio\": 100 * dataframe[col].value_counts() / len(dataframe),\n                                    \"Info\" : self.info[\"BU1 - BU24\"]}))\n\n            if plot:\n                \n                if col.startswith(\"BU\") is False or col.startswith(\"HU\") is False :\n                    groupped = dataframe.groupby(col,as_index=True)[col].count()\n\n                    \n\n                    fig = go.Figure(go.Bar(\n                        x=groupped.index,\n                        y = groupped.values),\n                        layout_title_text= info[col]\n                    )\n                    fig.show()\n                    # plt.figure(figsize=(7,7))\n                    # sns.countplot(x=dataframe[col], data=dataframe)\n                    \n\n                    # plt.title(info[col])\n                    # plt.show(block=True)\n                else:\n                    groupped = dataframe.groupby(col,as_index=True)[col].count()\n\n                    \n\n                    fig = go.Figure(go.Bar(\n                        x=groupped.index,\n                        y = groupped.values),\n                        layout_title_text= info[\"BU1 - BU24\"]\n                    )\n                    fig.show()\n                    \n                    # plt.figure(figsize=(10,10))\n                    # sns.countplot(x=dataframe[col], data=dataframe)\n                    \n\n                    # plt.title(info[\"BU1 - BU24\"])\n                    # plt.show(block=True)\n                ","metadata":{"execution":{"iopub.status.busy":"2024-04-08T18:36:51.743220Z","iopub.execute_input":"2024-04-08T18:36:51.743571Z","iopub.status.idle":"2024-04-08T18:36:51.770452Z","shell.execute_reply.started":"2024-04-08T18:36:51.743544Z","shell.execute_reply":"2024-04-08T18:36:51.769294Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"anl = Analysies(\"/kaggle/input/train-dataset/train.csv\",\n                \"/kaggle/input/test-dataset/test.csv\",\n                \"/kaggle/input/urun-ozellikleri/Urun Ozellikleri.xlsx\",\n                info)\n\n\nanl.trainData['SORU_GELIR_CVP'] = anl.trainData['SORU_GELIR_CVP'].apply(lambda x : x == x.split(\",\")[0] + \".\" + x.split(\",\")[1] if type(x) is str and ( \",\") in list(x) else x)  \nanl.trainData['SORU_GELIR_CVP'] = anl.trainData['SORU_GELIR_CVP'].astype(np.float32)\n\n\n\nanl.testData['SORU_GELIR_CVP'] = anl.testData['SORU_GELIR_CVP'].apply(lambda x : x == x.split(\",\")[0] + \".\" + x.split(\",\")[1] if type(x) is str and (\",\") in list(x) else x)  \nanl.testData['SORU_GELIR_CVP'] = anl.testData['SORU_GELIR_CVP'].astype(np.float32)\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T18:36:51.772912Z","iopub.execute_input":"2024-04-08T18:36:51.774184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols, num_cols, cat_but_car = anl.grab_col_names(anl.trainData)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols.remove(\"LABEL\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data = anl.trainData.drop([\"LABEL\"],axis = 1)\nchanged_data = x_data.drop(cat_but_car,axis=1)\ny_data = anl.trainData[[\"LABEL\"]]\ndef dropSomeCols(df:pd.DataFrame):\n\n    cols = []\n\n    for col in df.columns:\n        info = df[col].isna().value_counts()\n        info2 = pd.DataFrame({str(col) : [info.values[i]] for i,col in enumerate(info.index.values)})\n        if  info2[\"False\"].values < (int(len(df[col])/4)):\n            cols.append(col)\n\n    new_df = df.drop(cols,axis = 1)\n\n    print(f\"{cols} are dropped from df because almost all data is nan\")\n\n    return new_df,cols\n\ndroppedDf,cols = dropSomeCols(changed_data)\n\nnew_cat,new_num,new_cat_but_car = anl.grab_col_names(droppedDf)\n\n\nnum_keys = [\"ADET\",\"GELIR\",\"MIKTARI\",\"SAYISI\"]\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in new_cat:\n    try:\n\n\n        splitted = col.split(\"_\")\n        if (splitted[1] in num_keys):\n            new_num.append(col)\n            new_cat.remove(col)\n        elif (splitted[-1] in num_keys):\n            new_num.append(col)\n            new_cat.remove(col)\n        elif (splitted[-2] in num_keys):\n            new_num.append(col)\n            new_cat.remove(col)\n    except:\n        splitted = col\n        continue\n\nprint(new_cat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = anl.testData\ncat_data = test_data[new_cat]\nnum_data = test_data[new_num]\n\nfor col in new_cat:\n\n    cat_data[col] = cat_data[col].astype(\"O\")\n\nfor col in new_cat:\n    cat_data[col] = cat_data[col].fillna(cat_data[col].mode().iloc[0])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knnImpute = SimpleImputer(strategy = \"mean\")\nimputed = knnImpute.fit_transform(droppedDf[new_num])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = pd.DataFrame({cols : imputed.T[i] for i,cols in enumerate(new_num)})\nx_data = droppedDf.drop(new_df.columns,axis = 1)\nx_data = pd.concat([new_df,x_data],axis = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfor col in new_cat:\n    x_data[col] = x_data[col].fillna(x_data[col].mode().iloc[0])\n\n\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outlier_thresholds(df, col_name, q1=0.33, q3=0.66):\n    quartile1 = df[col_name].quantile(q1)\n    quartile3 = df[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_thresholds(x_data,y_data,variable):\n    low_limit, up_limit = outlier_thresholds(x_data, variable)\n    \n    indexes_list = []\n    \n    \n    if len(x_data.loc[(x_data[variable] < low_limit), variable].index.values) != 0 :\n        \n        indexes_list.extend(list(x_data.loc[(x_data[variable] < low_limit), variable].index.values))\n    elif len(x_data.loc[(x_data[variable]>up_limit),variable].index.values) !=0:\n        indexes_list.extend(list(x_data.loc[(x_data[variable]>up_limit),variable].index.values))\n        \n    \n    if len(indexes_list)>0:\n        \n    \n        \n        willDelete = y_data.iloc[indexes_list] == \"UA\"\n        \n        willDelete = list(willDelete[willDelete[\"LABEL\"] == True].index.values)\n        \n        print(\"#################################\")\n        \n        \n        x_data = x_data.drop(willDelete,axis=0).reset_index(drop=True)\n        y_data = y_data.drop(willDelete,axis = 0).reset_index(drop = True)\n        \n        \n    x_data.loc[(x_data[variable] < low_limit), variable] = low_limit\n    x_data.loc[(x_data[variable] > up_limit), variable] = up_limit\n    \n    indexes_list.clear()\n    \n    return x_data,y_data\n    \n    \n    \n\n\ndef check_outlier(df, col_name):\n    low_limit, up_limit = outlier_thresholds(df, col_name)\n    if df[(df[col_name] > up_limit) | (df[col_name] < low_limit)].any(axis=None):\n        \n        print(f\"{col_name} : True\")\n        return True\n    else:\n        print(f\"{col_name} : False\")\n        return False\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in new_num:\n    print(col)\n    x_data,y_data = replace_with_thresholds(x_data,y_data,col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_test = knnImpute.transform(num_data)\nimputed_df_test = pd.DataFrame({col : imputed_test.T[i] for i,col in enumerate(new_num)})\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor col in new_cat:\n\n    x_data[col] = x_data[col].astype(\"O\")\n\n\n\nohe = OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\n\ntransformed_x = ohe.fit_transform(x_data[new_cat])\nprint(transformed_x)\n\n\nencoded_x = pd.DataFrame({col : transformed_x.T[i] for i,col in enumerate(ohe.get_feature_names_out(new_cat))})\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in new_cat:\n    cat_data[col] = cat_data[col].fillna(cat_data[col].mode().iloc[0])\n\n    \n\ntransformed_test = ohe.fit_transform(cat_data)\nencoded_test = pd.DataFrame({col : transformed_test.T[i] for i,col in enumerate(ohe.get_feature_names_out(cat_data.columns))})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nle = LabelEncoder()\n\ny_data = le.fit_transform(y_data)\n\nnew_x = x_data.drop(new_cat,axis= 1)\nnew_x = pd.concat([new_x,encoded_x],axis = 1)\nnew_x = new_x.drop(new_cat_but_car,axis = 1)\n\n\nscaler = RobustScaler()\n\n\nscaled_x = scaler.fit_transform(new_x[new_num],y_data)\nnew_x.drop(new_num,axis=1,inplace=True)\nscaled_x = pd.DataFrame({cols : scaled_x.T[i] for i,cols in enumerate(new_num)})\n\nnew_x = pd.concat([new_x,scaled_x],axis=1)\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_test_scaled = scaler.transform(imputed_df_test)\n\nnum_test_scaled_df = pd.DataFrame({cols : num_test_scaled.T[i] for i,cols in enumerate(new_num)})\n\nprocessed_test_data = pd.concat([encoded_test,num_test_scaled_df],axis = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_test_new, num_test_new, car_test_new = anl.grab_col_names(processed_test_data)\ncat_train_new,num_train_new,car_train_new = anl.grab_col_names(new_x)\n\nwill_add_in_train = [col  for col in cat_test_new if col not in cat_train_new]\nwill_add_in_test = [col for col in cat_train_new if col not in cat_test_new ]\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"####################################\")\n\nprint(f\"Columns which will add in traindata {will_add_in_train}\")\nprint(\"####################################\")\nprint(f\"Columns which will add in testdata {will_add_in_test}\")\n\nzero_matrix_train = np.zeros((len(new_x),1))\nzero_matrix_test = np.zeros((len(processed_test_data),1))\n\n\nfor col in will_add_in_train:\n    new_x[col] = zero_matrix_train\n\nfor col in will_add_in_test:\n    processed_test_data[col] = zero_matrix_test\n\nprocessed_test_data = processed_test_data[new_x.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# num_classes = anl.trainData[\"LABEL\"].nunique()\n\n# param_grid = {\n#     \"XGBM\": {\n#         \"objective\" : [\"multi:softprob\"],\n#         'gamma': [0.5, 1, 1.5, 2, 5],\n#         \"num_class\" : [num_classes],\n#         \"booster\" : [\"gbtree\",\"dart\"],\n#         \"eta\" : np.linspace(1e-3,0.1,3),\n#         \"max_depth\" :  np.arange(5,18,4),\n#         \"subsample\" : np.linspace(0.5,1.,3),\n#         \"colsample_bytree\" : np.linspace(0.5,1.,3),\n#         \"n_estimators\" : [180,350,500,800]\n#     },\n\n#     \"LGBM\" : {\n#         \"objective\" : [\"multi_class\"],\n#         \"boosting_type\" : [\"gbdt\",\"goss\",\"dart\"],\n#         \"max_depth\" : np.arange(5,18,4),\n#         \"colsample_bytree\" :  np.linspace(0.5,1.,3),\n#         \"subsample\" : np.linspace(0.5,1.,3),\n#         \"learning_rate\" : np.linspace(1e-3,0.1,4),\n#         # \"class_weight\" : {[0,1,2,3,4,5,6,7] : []},\n#         \"reg_lambda\" : np.linspace(0.,1.,3),\n#         \"importance_type\" : [\"split\",\"gain\"],\n#         \"num_classes\" : [num_classes]\n        \n        \n#         }\n \n# }\n\n\n# models = {\n#     \"LGBM\" : LGBMClassifier(),\n#     \"XGBM\" : XGBClassifier()\n    \n# }\n\nclasses = [0, 1, 2, 3, 4, 5, 6, 7]\nweights = [0.0385, 0.0328, 0.2791, 0.1812, 0.0113, 0.2952, 0.1614, 0.0001]\n\ndef competition_metric(y_true, y_pred):\n    \n    return np.sum(f1_score(y_true, y_pred, average=None, labels=classes) * weights)\n\n\n# bests = {\n\n#     }\n\n# def train(params:dict,model,trainX:pd.DataFrame,trainY:pd.DataFrame):\n\n    \n\n        \n#     cv = RandomizedSearchCV(models[model],params,scoring= custom_scorer,cv = 5,refit = True, verbose = 10,error_score = \"raise\").fit(trainX,trainY)\n    \n\n    \n            \n            \n        \n#     return cv.best_params_\n\n\n\n# for model in list(models.keys()):\n\n#     bests[model] = train(params = param_grid[model],model = model,trainX = new_x, trainY=y_data)\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bests = {'subsample': 0.5,\n  'reg_lambda': 1.0,\n  'objective': 'multiclass',\n  'num_class': 8,\n  'max_depth': 17,\n  'learning_rate': 0.034,\n  'importance_type': 'split',\n  'colsample_bytree': 0.75,\n  'boosting_type': 'goss'}\n\n# Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier()\n\n# Train the Random Forest classifier on the training data\nrf_classifier.fit(new_x, y_data)\n\n# Predict the labels for the test set\ny_pred = rf_classifier.predict(processed_test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(r\"/kaggle/input/sample-submission/sample_submission.csv\")\n\nfinal_submission = pd.concat([anl.testData[\"MUSTERI_ID\"], pd.DataFrame(y_pred, columns=['LABEL'])], axis=1)\n\n# Define the mapping dictionary\nlabel_mapping = {\n    0: 'HU06',\n    1: 'HU07',\n    2: 'HU11',\n    3: 'HU12',\n    4: 'HU14',\n    5: 'HU15',\n    6: 'HU19',\n    7: 'UA'\n}\n\n# Convert the \"LABEL\" column to string labels\nfinal_submission['LABEL'] = final_submission['LABEL'].map(label_mapping).astype(str)\n\nfinal_submission.to_csv(r'/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}